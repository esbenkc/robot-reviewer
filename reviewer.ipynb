{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Robot Reviewer ðŸ¤–\r\n",
    "Here, we download a HuggingFace library model and use OCR on PDFs to drag out relevant content. The goal is to take any number of PDFs (even folders of them) and mark the most interesting parts using an array of settings that makes it clear what you're looking for, e.g. size of output edges, specific search terms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the PDF\r\n",
    "import textract\r\n",
    "import re\r\n",
    "import spacy\r\n",
    "import numpy as np\r\n",
    "# Load English tokenizer, tagger, parser and NER\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MostSimilar(word, topn=5):\r\n",
    "    word = nlp.vocab[str(word)]\r\n",
    "    queries = [\r\n",
    "        w for w in word.vocab \r\n",
    "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\r\n",
    "    ]\r\n",
    "\r\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\r\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\r\n",
    "\r\n",
    "def Pipeline(pdf_path):\r\n",
    "    if \".pdf\" not in pdf_path:\r\n",
    "        print(\"Not a .pdf file\")\r\n",
    "        return None\r\n",
    "\r\n",
    "    # Extract text from PDF\r\n",
    "    text = textract.process(pdf_path)\r\n",
    "    text = text.decode(\"utf-8\")\r\n",
    "    # Remove \\r and \\n\r\n",
    "    text = re.sub(\"\\\\n\\\\r\\\\t\", \" \", text)\r\n",
    "    return text\r\n",
    "\r\n",
    "def Reviewer(keywords, pdf_path):\r\n",
    "    text = Pipeline(pdf_path)\r\n",
    "    result = nlp(text)\r\n",
    "\r\n",
    "    for word in keywords:\r\n",
    "        print(MostSimilar(word))\r\n",
    "\r\n",
    "\r\n",
    "    return result\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc = Reviewer([\"AI\", \"game\"], \"test.pdf\")\r\n",
    "\r\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\r\n",
    "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\r\n",
    "\r\n",
    "# for entity in doc.ents:\r\n",
    "#     print(entity.text, entity.label_)\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}